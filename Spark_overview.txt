links:
https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example
https://sparkbyexamples.com/pyspark-tutorial/




chapter1:

1-Launching the Python console
You’ll need Python 2 or 3 installed in order to launch the Python console. From Spark’s home
directory, run the following code:

$ ./bin/pyspark
After you’ve done that, type “spark” and press Enter. You’ll see the SparkSession object printed,

2-Launching the Scala console
To launch the Scala console, you will need to run the following command:

$ ./bin/spark-shell

After you’ve done that, type “spark” and press Enter. As in Python, you’ll see the SparkSession

3-Launching the SQL console
Parts of this book will cover a large amount of Spark SQL. For those, you might want to start the SQL
console.

$ ./bin/spark-sql





chapter2:

To read this data,
we will use a DataFrameReader that is associated with our SparkSession. In doing so, we will

(1)specify the file format as well as any options we want to specify. In our case, we want to do
something called schema inference, which means that we want Spark to take a best guess at what the schema of our DataFrame should be.

(2)We also want to specify that the first row is the header in the
file, so we’ll specify that as an option, too.

To get the schema information, Spark reads in a little bit of the data and then attempts to parse the
types in those rows according to the types available in Spark. You also have the option of strictly
specifying a schema when you read in data (which we recommend in production scenarios):

# in Python
flightData2015 = spark\
.read\
.option("inferSchema", "true")\
.option("header", "true")\
.csv("/data/flight-data/csv/2015-summary.csv")


If we perform the take action on the DataFrame, we will be able to see the same results that we saw
before when we used the command line:

# flightData2015.take(3)


Nothing happens to the data when we call sort because it’s just a transformation. However, we can
see that Spark is building up a plan for how it will execute this across the cluster by looking at the
explain plan. We can call explain on any DataFrame object to see the DataFrame’s lineage (or how
Spark will execute this query):

#flightData2015.sort("count").explain()




Now, just like we did before, we can specify an action to kick off this plan. However, before doing
that, we’re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200
shuffle partitions. Let’s set this value to 5 to reduce the number of the output partitions from the
shuffle:

#spark.conf.set("spark.sql.shuffle.partitions", "5")


With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure
SQL. There is no performance difference between writing SQLqueries or writing DataFrame code,

You can make any DataFrame into a table or view with one simple method call:

#flightData2015.createOrReplaceTempView("flight_data_2015")

Now we can query our data in SQL.


# in Python

sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")

dataFrameWay = flightData2015\
.groupBy("DEST_COUNTRY_NAME")\
.count()

sqlWay.explain()
dataFrameWay.explain()

Notice that these plans compile to the exact same underlying plan!



he max function, to establish the maximum number of flights to and from any given location. This just
scans each value in the relevant column in the DataFrame and checks whether it’s greater than the
previous values that have been seen.

#sql:
spark.sql("SELECT max(count) from flight_data_2015").take(1)

#python:
from pyspark.sql.functions import max
flightData2015.select(max("count")).take(1)




Great, that’s a simple example that gives a result of 370,002. Let’s perform something a bit more
complicated and find the top five destination countries in the data. This is our first multitransformation query, so we’ll take it step by step. Let’s begin with a fairly straightforward SQL
aggregation:

# sql
maxSql = spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count) DESC
LIMIT 5
""")
maxSql.show()



# in Python
from pyspark.sql.functions import desc
flightData2015\
.groupBy("DEST_COUNTRY_NAME")\
.sum("count")\
.withColumnRenamed("sum(count)", "destination_total")\
.sort(desc("destination_total"))\
.limit(5)\
.show()


In general, many DataFrame methods will
accept strings (as column names) or Column types or expressions. Columns and expressions are
actually the exact same thing.











Now, start the Spark history server on Linux or Mac by running.
$SPARK_HOME/sbin/start-history-server.sh


By default, the History server listens at 18080 port and you can access it from the browser using http://localhost:18080/





#SparkSession is an entry point to underlying Spark functionality in order to programmatically use Spark RDD, DataFrame, and Dataset
#SparkSession will be created using SparkSession.builder() builder pattern

spark = SparkSession.builder \
    .master("local[1]") \
    .appName("SparkByExamples.com") \
    .getOrCreate()

schema = StructType([
    StructField("language", StringType(), True),
    StructField("users_count", StringType(), True)
])

# Create a DataFrame with the provided data and schema
data = [("Java", "20000"), ("Python", "100000"), ("Scala", "3000")]
df = spark.createDataFrame(data, schema=schema)

# Show the DataFrame
df.show()
*/


/home/omar/virtualenv/bin/python3

